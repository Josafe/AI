{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 50 primeres ressenyes tokenitzades: [[0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        output = self.fc(hn[-1])\n",
    "        return output\n",
    "\n",
    "# Funció per netejar el text\n",
    "def netejar_text(text):\n",
    "    text = text.lower()  # Convertir tot a minúscules\n",
    "    text = ''.join([caracter for caracter in text if caracter.isalpha() or caracter == ' '])  \n",
    "    # Conservar només lletres i espais\n",
    "    return text\n",
    "\n",
    "# Llegir i processar el CSV\n",
    "def processar_csv(ruta_csv):\n",
    "    ressenyes = []\n",
    "    etiquetes = []\n",
    "\n",
    "    with open(ruta_csv, mode='r', encoding='utf-8') as arxiu:\n",
    "        lector_csv = csv.reader(arxiu)\n",
    "        next(lector_csv)  # Saltar la primera línia (encapçalament)\n",
    "        for fila in lector_csv:\n",
    "            etiqueta = None\n",
    "            if fila[0] == 'positive':\n",
    "                etiqueta = 1\n",
    "            elif fila[0] == 'negative':\n",
    "                etiqueta = 0\n",
    "            \n",
    "            ressenya = netejar_text(fila[1])  # Neteja del text a la columna 1\n",
    "            ressenyes.append(ressenya)\n",
    "            etiquetes.append(etiqueta)\n",
    "        \n",
    "        return ressenyes, etiquetes\n",
    "\n",
    "\n",
    "# Tokenitzar les ressenyes\n",
    "def tokenitzar_ressenyes(ressenyes):\n",
    "    paraula_a_index = {}\n",
    "    ressenyes_tokenitzades = []\n",
    "    index = 0\n",
    "\n",
    "    for ressenya in ressenyes:\n",
    "        tokens = ressenya.split()\n",
    "        tokenitzada = []\n",
    "        for paraula in tokens:\n",
    "            if paraula not in paraula_a_index:\n",
    "                paraula_a_index[paraula] = index\n",
    "                index += 1\n",
    "            tokenitzada.append(paraula_a_index[paraula])\n",
    "        ressenyes_tokenitzades.append(tokenitzada)\n",
    "    return ressenyes_tokenitzades, paraula_a_index\n",
    "\n",
    "# Ruta de l'arxiu CSV\n",
    "ruta_csv = \"/home/itibcn/Desktop/Torch/datasets/IMDB/IMDBDataset.csv\"\n",
    "\n",
    "# Processar les dades del CSV\n",
    "ressenyes, etiquetes = processar_csv(ruta_csv)\n",
    "if ressenyes:\n",
    "    ressenyes_tokenitzades, paraula_a_index = tokenitzar_ressenyes(ressenyes)\n",
    "    print(f\"Les 50 primeres ressenyes tokenitzades: {ressenyes_tokenitzades[:50]}\")\n",
    "else:\n",
    "    print(\"No s'han pogut processar dades.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m     ressenyes_padded \u001b[38;5;241m=\u001b[39m padding_sequencies(ressenyes_tokenitzades, max_len)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Convertir a tensors\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mressenyes_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(etiquetes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# --- Pas 2: Dataset Personalitzat ---\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Pas 1: Configuració i preprocessing ---\n",
    "def netejar_text(text):\n",
    "    # Convertir a minúscules, treure símbols especials\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Eliminar caràcters no alfanumèrics\n",
    "    text = text.strip()  # Eliminar espais innecessaris\n",
    "    return text\n",
    "\n",
    "\n",
    "def veure_prediccions(model, dataloader, paraula_a_index):\n",
    "    model.eval()\n",
    "    idx_a_paraula = {index: paraula for paraula, index in paraula_a_index.items()} \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            output = model(X_batch).to(device)\n",
    "            pred = (output.squeeze() > 0.5).float()\n",
    "            for seq, etiqueta, prediccio in zip(X_batch, y_batch, pred):\n",
    "                # Convertir la seqüència d'índexs a text\n",
    "                ressenya = \" \".join([idx_a_paraula[idx.item()] for idx in seq if idx.item() in idx_a_paraula])\n",
    "                etiqueta_real = \"Positive\" if etiqueta.item() == 1 else \"Negative\"\n",
    "                prediccio_str = \"Positive\" if prediccio.item() == 1 else \"Negative\"\n",
    "                print(f\"Ressenya: {ressenya}\")\n",
    "                print(f\"Etiqueta Real: {etiqueta_real}, Predicció: {prediccio_str}\\n\")\n",
    "    \n",
    "    \n",
    "    # Llegir i processar el CSV\n",
    "def processar_csv(ruta_csv):\n",
    "    ressenyes = []\n",
    "    etiquetes = []\n",
    "\n",
    "    with open(ruta_csv, mode='r', encoding='utf-8') as arxiu:\n",
    "        lector_csv = csv.reader(arxiu)\n",
    "        next(lector_csv)  # Saltar la primera línia \n",
    "\n",
    "        for fila in lector_csv:\n",
    "            if len(fila) < 2:\n",
    "                # La fila no té suficients columnes\n",
    "                print(f\"Fila incorrecta: {fila}\")\n",
    "                continue\n",
    "            \n",
    "            etiqueta = None\n",
    "            if fila[1] == 'positive':\n",
    "                etiqueta = 1\n",
    "            elif fila[1] == 'negative':\n",
    "                etiqueta = 0\n",
    "\n",
    "            # Comprovar si l'etiqueta és vàlida\n",
    "            if etiqueta is None:\n",
    "                print(f\"Etiqueta no vàlida en fila: {fila}\")\n",
    "                continue\n",
    "\n",
    "            ressenya = netejar_text(fila[1])\n",
    "            if not ressenya.strip():\n",
    "                print(f\"Ressenya buida en fila: {fila}\")\n",
    "                continue\n",
    "\n",
    "            ressenyes.append(ressenya)\n",
    "            etiquetes.append(etiqueta)\n",
    "\n",
    "    return ressenyes, etiquetes\n",
    "\n",
    "\n",
    "# Potser s'hauria de cridar aquest mètode un cop per fora\n",
    "def tokenitzar_ressenyes(ressenyes):\n",
    "    \"\"\"\n",
    "    Convertim les ressenyes en tokens mitjançant un diccionari.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    paraula_a_index = {}\n",
    "    index = 0\n",
    "    for ressenya in ressenyes:\n",
    "        seq = []\n",
    "        for paraula in ressenya.split():\n",
    "            if paraula not in paraula_a_index:\n",
    "                paraula_a_index[paraula] = index\n",
    "                index += 1\n",
    "            seq.append(paraula_a_index[paraula])\n",
    "        tokens.append(seq)\n",
    "    return tokens, paraula_a_index\n",
    "\n",
    "def padding_sequencies(sequencies, max_len):\n",
    "    padded = np.zeros((len(sequencies), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequencies):\n",
    "        padded[i, :len(seq)] = seq[:max_len]\n",
    "    return padded\n",
    "\n",
    "# Processar les dades\n",
    "max_len = 100  # Longitud màxima de les ressenyes (padding)\n",
    "ressenyes, etiquetes = processar_csv(ruta_csv)\n",
    "if len(ressenyes) > 0:\n",
    "    ressenyes_tokenitzades, paraula_a_index = tokenitzar_ressenyes(ressenyes)\n",
    "    ressenyes_padded = padding_sequencies(ressenyes_tokenitzades, max_len)\n",
    "\n",
    "# Convertir a tensors\n",
    "X = torch.tensor(ressenyes_padded, dtype=torch.long).to(device)\n",
    "y = torch.tensor(etiquetes, dtype=torch.float).to(device)\n",
    "\n",
    "# --- Pas 2: Dataset Personalitzat ---\n",
    "class RessenyesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = RessenyesDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# --- Pas 3: Definició del Model ---\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # Per classificar entre 0 i 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Embeddings per a la seqüència d'índexs\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        output = self.fc(hn[-1])  # Últim estat ocult\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retorna un parell (X[idx], y[idx]) que correspon a la ressenya i la seva etiqueta.\n",
    "        :param idx: Índex de l'element a retornar.\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Paràmetres del model\n",
    "vocab_size = len(paraula_a_index)\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1  # 1 classe (positiu o negatiu)\n",
    "dropout = 0.5\n",
    "\n",
    "model = NeuralNetwork(vocab_size, embed_size, hidden_size, num_layers, output_size, dropout).to(device)\n",
    "\n",
    "# --- Pas 4: Entrenament del Model ---\n",
    "# Paràmetres de l'entrenament\n",
    "criteri = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimitzador = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimitzador.zero_grad()\n",
    "        output = model(X_batch).to(device)\n",
    "        loss = criteri(output.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimitzador.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Època {epoch+1}/{epochs}, Pèrdua: {total_loss/len(dataloader)}\")\n",
    "\n",
    "# --- Pas 5: Avaluació ---\n",
    "def accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            output = model(X_batch).to(device)\n",
    "            pred = (output.squeeze() > 0.5).float()\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    return correct / total\n",
    "\n",
    "print(f\"Precisió del model: {accuracy(model, dataloader):.2f}\")\n",
    "\n",
    "\n",
    "# Mostra algunes prediccions del model\n",
    "veure_prediccions(model, dataloader, paraula_a_index)\n",
    "\n",
    "\n",
    "model_path = '/home/itibcn/Desktop/Torch/modelRessenyesCine.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoVirtualPytorch",
   "language": "python",
   "name": "entornovirtualpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
