{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers amb Tokenizer manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Preprocessat del text ---\n",
    "def netejar_text(text):\n",
    "    text = text.lower() #Text a minuscules\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Eliminar caràcters no alfanumèrics\n",
    "    return text.strip()  # Eliminar espais innecessaris\n",
    "\n",
    "def processar_csv(ruta_csv):\n",
    "    ressenyes, etiquetes = [], [] #Inicialitzar variables llistes\n",
    "\n",
    "    with open(ruta_csv, mode='r', encoding='utf-8') as arxiu: #Obrir amb mode lectura i codificacio utf8\n",
    "        lector_csv = csv.reader(arxiu) #Llegix arxiu linea a linea\n",
    "        next(lector_csv)  # Saltar la capçalera\n",
    "        for fila in lector_csv: #Iterarem amb el bucle for \n",
    "            if len(fila) < 2: #Filtrarem les linees incorrectes\n",
    "                continue\n",
    "            etiqueta = 1 if fila[1] == 'positive' else 0 #Llegeix la segona columna (fila[1]) per determinar l'etiqueta, si es 1 sera positiva si es 0 negativa\n",
    "            ressenya = netejar_text(fila[0]) #Llegeix la primera columna (fila[0]) i netejem els espais innecessaris\n",
    "            ressenyes.append(ressenya) #Guarda la ressenya del CSV\n",
    "            etiquetes.append(etiqueta) #Guarda l'etiqueta (1 o 0) a la llista etiquetes\n",
    "    return ressenyes, etiquetes #Ho retornem al acabar el metode\n",
    "\n",
    "def tokenitzar_ressenyes(ressenyes):\n",
    "    paraula_a_index = {} #Inicialitzar diccionari\n",
    "    tokens = [] #Conté la sequencia numerica de cada ressenya\n",
    "    index = 0 #Comptador per assignar indexs unics\n",
    "    for ressenya in ressenyes: #Recorre la llista de ressenyes retornada a la funcio de processar_csv\n",
    "        seq = [] #Llista temporal per a emmagatzemar la sequencia d'indexs de la ressenya\n",
    "        for paraula in ressenya.split(): #Es divideix la ressenya en paraules\n",
    "            if paraula not in paraula_a_index: #Si una paraula encara no està al diccionari paraula_a_index, se li assigna l'índex.\n",
    "                paraula_a_index[paraula] = index\n",
    "                index += 1\n",
    "            seq.append(paraula_a_index[paraula]) #L'índex de la paraula (ja sigui nou o existent) s'afegeix a la llista seq.\n",
    "        tokens.append(seq) #Despres de processar les paraules de la ressenya la seqüencia numerica(token) s'afegeix a tokens\n",
    "    return tokens, paraula_a_index\n",
    "\n",
    "def padding_sequencies(sequencies, max_len):\n",
    "    padded = np.zeros((len(sequencies), max_len), dtype=int)\n",
    "    for i, seq in enumerate(sequencies):\n",
    "        padded[i, :len(seq)] = seq[:max_len]\n",
    "    return padded\n",
    "\n",
    "def veure_prediccions(model, dataloader, paraula_a_index):\n",
    "    model.eval()\n",
    "    idx_a_paraula = {index: paraula for paraula, index in paraula_a_index.items()} \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            output = model(X_batch).to(device)\n",
    "            pred = (output.squeeze() > 0.5).float()\n",
    "            for seq, etiqueta, prediccio in zip(X_batch, y_batch, pred):\n",
    "                # Convertir la seqüència d'índexs a text\n",
    "                ressenya = \" \".join([idx_a_paraula[idx.item()] for idx in seq if idx.item() in idx_a_paraula])\n",
    "                etiqueta_real = \"Positive\" if etiqueta.item() == 1 else \"Negative\"\n",
    "                prediccio_str = \"Positive\" if prediccio.item() == 1 else \"Negative\"\n",
    "                print(f\"Ressenya: {ressenya}\")\n",
    "                print(f\"Etiqueta Real: {etiqueta_real}, Predicció: {prediccio_str}\\n\")\n",
    "# --- Dataset personalitzat ---\n",
    "class RessenyesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# --- Model Transformer ---\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, output_size, max_len, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, embed_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.positional_encoding[:x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# --- Entrenament del Model ---\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X).squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size, num_batches, test_loss, correct = len(dataloader.dataset), len(dataloader), 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5).float() == y).sum().item()\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct / size):.1f}%, Avg loss: {test_loss / num_batches:.4f}\\n\")\n",
    "\n",
    "# --- Paràmetres i procés ---\n",
    "ruta_csv = \"/home/itibcn/Desktop/Torch/datasets/IMDB/IMDBDataset.csv\"\n",
    "max_len = 100\n",
    "embed_size, num_heads, hidden_size, num_layers, dropout = 128, 8, 128, 2, 0.5\n",
    "batch_size, learning_rate, epochs = 32, 0.001, 100\n",
    "\n",
    "ressenyes, etiquetes = processar_csv(ruta_csv)\n",
    "ressenyes_tokenitzades, paraula_a_index = tokenitzar_ressenyes(ressenyes)\n",
    "ressenyes_padded = padding_sequencies(ressenyes_tokenitzades, max_len)\n",
    "\n",
    "X = torch.tensor(ressenyes_padded, dtype=torch.long)\n",
    "y = torch.tensor(etiquetes, dtype=torch.float)\n",
    "dataset = RessenyesDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vocab_size = len(paraula_a_index)\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, 1, max_len, dropout).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Entrenament finalitzat.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers amb BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Crear el tokenitzador\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenitzar i paddar les ressenyes\n",
    "def tokenitzar_amb_bert(ressenyes, max_len):\n",
    "    tokenized_data = tokenizer(\n",
    "        ressenyes,\n",
    "        padding='max_length',    # Afegeix padding fins a max_len\n",
    "        truncation=True,         # Retalla seqüències més llargues que max_len\n",
    "        max_length=max_len,      # Màxim nombre de tokens per seqüència\n",
    "        return_tensors=\"pt\"      # Retorna tensors per a PyTorch\n",
    "    )\n",
    "    return tokenized_data['input_ids'], tokenized_data['attention_mask']\n",
    "\n",
    "#Modificar dataset\n",
    "class RessenyesDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, y):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx], self.y[idx]\n",
    "\n",
    "#Modificar el model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, output_size, max_len, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, embed_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        #print(f\"x shape before embedding: {x.shape}\")  # (batch_size, seq_len)\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        #print(f\"x shape after embedding: {x.shape}\")  # (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        #if attention_mask is not None:\n",
    "            #print(f\"Attention mask shape: {attention_mask.shape}\")  # (batch_size, seq_len)\n",
    "\n",
    "        x = self.transformer_encoder(x.permute(1, 0, 2), src_key_padding_mask=~attention_mask.bool())\n",
    "        #print(f\"x shape after transformer: {x.shape}\")  # (seq_len, batch_size, embed_size)\n",
    "        \n",
    "        x = x.permute(1, 0, 2).mean(dim=1)  # Agafem la mitjana al llarg de seq_len\n",
    "        #print(f\"x shape after mean: {x.shape}\")  # (batch_size, embed_size)\n",
    "        \n",
    "        return self.sigmoid(self.fc(x))\n",
    "\n",
    "\n",
    "#Preprocessar les dades\n",
    "def netejar_text(text):\n",
    "    text = text.lower() #Text a minuscules\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Eliminar caràcters no alfanumèrics\n",
    "    return text.strip()  # Eliminar espais innecessaris\n",
    "\n",
    "def processar_csv(ruta_csv):\n",
    "    ressenyes, etiquetes = [], [] #Inicialitzar variables llistes\n",
    "\n",
    "    with open(ruta_csv, mode='r', encoding='utf-8') as arxiu: #Obrir amb mode lectura i codificacio utf8\n",
    "        lector_csv = csv.reader(arxiu) #Llegix arxiu linea a linea\n",
    "        next(lector_csv)  # Saltar la capçalera\n",
    "        for fila in lector_csv: #Iterarem amb el bucle for \n",
    "            if len(fila) < 2: #Filtrarem les linees incorrectes\n",
    "                continue\n",
    "            etiqueta = 1 if fila[1] == 'positive' else 0 #Llegeix la segona columna (fila[1]) per determinar l'etiqueta, si es 1 sera positiva si es 0 negativa\n",
    "            ressenya = netejar_text(fila[0]) #Llegeix la primera columna (fila[0]) i netejem els espais innecessaris\n",
    "            ressenyes.append(ressenya) #Guarda la ressenya del CSV\n",
    "            etiquetes.append(etiqueta) #Guarda l'etiqueta (1 o 0) a la llista etiquetes\n",
    "    return ressenyes, etiquetes #Ho retornem al acabar el metode\n",
    "\n",
    "ruta_csv = \"/home/itibcn/Desktop/Torch/datasets/IMDB/IMDBDataset.csv\"\n",
    "embed_size, num_heads, hidden_size, num_layers, dropout = 128, 8, 128, 2, 0.5\n",
    "batch_size, learning_rate, epochs = 32, 0.001, 100\n",
    "max_len = 100  # Defineix el valor abans d'utilitzar-lo\n",
    "ressenyes, etiquetes = processar_csv(ruta_csv)\n",
    "\n",
    "input_ids, attention_mask = tokenitzar_amb_bert(ressenyes, max_len)\n",
    "\n",
    "# Convertir a tensores\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "y = torch.tensor(etiquetes, dtype=torch.float).to(device)\n",
    "\n",
    "dataset = RessenyesDataset(input_ids, attention_mask, y)\n",
    "dataloader = DataLoader(dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "#Entrenar el model\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for input_ids, attention_mask, y in dataloader:\n",
    "        input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
    "        pred = model(input_ids, attention_mask).squeeze()\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size, test_loss, correct = len(dataloader.dataset), 0, 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, y in dataloader:\n",
    "            input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
    "            pred = model(input_ids, attention_mask).squeeze()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5).float() == y).sum().item()\n",
    "    print(f\"Accuracy: {(100 * correct / size):.1f}%, Avg loss: {test_loss / len(dataloader):.4f}\\n\")\n",
    "\n",
    "#Llançar el model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, 1, max_len, dropout).to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Entrenament finalitzat.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entornoVirtualPytorch",
   "language": "python",
   "name": "entornovirtualpytorch"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
